facet_wrap(~ topic, scales = "free", ncol = 4) +
scale_y_reordered() +
theme_tufte(base_family = "Helvetica")
tidy_donhar <- donhar %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
## Count most common words in both
tidy_donhar %>%
count(word, sort = TRUE)
bookfreq <- tidy_donhar %>%
mutate(author = ifelse(gutenberg_id==53368, "Donovan", "Hardy")) %>%
mutate(word = str_extract(word, "[a-z']+")) %>%
count(author, word) %>%
group_by(author) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(author, proportion)
ggplot(bookfreq, aes(x = Donovan, y = Hardy, color = abs(Donovan - Hardy))) + ##absolute freq difference between Don and Har
geom_abline(color = "red", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "red") +
theme_tufte(base_family = "Helvetica") +
theme(legend.position="none",
strip.background = element_blank(),
strip.text.x = element_blank()) +
labs(x = "Donovan", y = "Hardy") +
coord_equal()
donhar <- donhar %>%
filter(!is.na(text))
# Divide into documents, each representing one chapter
donhar_chapter <- donhar %>%
mutate(author = ifelse(gutenberg_id==53368, "Donovan", "Hardy")) %>%
group_by(author) %>%
mutate(chapter = cumsum(str_detect(text, regex("^chapter ", ignore_case = TRUE)))) %>%
ungroup() %>%
filter(chapter > 0) %>%
unite(document, author, chapter)
# Split into words
donhar_chapter_word <- donhar_chapter %>%
unnest_tokens(word, text)
# Find document-word counts
donhar_word_counts <- donhar_chapter_word %>%
anti_join(stop_words) %>%
count(document, word, sort = TRUE) %>%
ungroup()
donhar_word_counts
# Cast into DTM format for LDA analysis
donhar_chapters_dtm <- donhar_word_counts %>%
cast_dtm(document, word, n)
tm::inspect(donhar_chapters_dtm)
donhar_chapters_lda <- LDA(donhar_chapters_dtm, k = 2, control = list(seed = 1234))
donhar_chapters_gamma <- tidy(donhar_chapters_lda, matrix = "gamma")
donhar_chapters_gamma
# First separate the document name into title and chapter
donhar_chapters_gamma <- donhar_chapters_gamma %>%
separate(document, c("title", "chapter"), sep = "_", convert = TRUE)
donhar_chapter_classifications <- donhar_chapters_gamma %>%
group_by(title, chapter) %>%
top_n(1, gamma) %>%
ungroup()
donhar_book_topics <- donhar_chapter_classifications %>%
count(title, topic) %>%
group_by(title) %>%
top_n(1, n) %>%
ungroup() %>%
transmute(consensus = title, topic)
donhar_chapter_classifications %>%
inner_join(donhar_book_topics, by = "topic") %>%
filter(author != consensus)
# First separate the document name into title and chapter
donhar_chapters_gamma <- donhar_chapters_gamma %>%
separate(document, c("title", "chapter"), sep = "_", convert = TRUE)
donhar_chapters_gamma <- donhar_chapters_gamma %>%
separate(document, c("title", "chapter"), sep = "_", convert = TRUE)
donhar <- donhar %>%
filter(!is.na(text))
# Divide into documents, each representing one chapter
donhar_chapter <- donhar %>%
mutate(author = ifelse(gutenberg_id==53368, "Donovan", "Hardy")) %>%
group_by(author) %>%
mutate(chapter = cumsum(str_detect(text, regex("^chapter ", ignore_case = TRUE)))) %>%
ungroup() %>%
filter(chapter > 0) %>%
unite(document, author, chapter)
# Split into words
donhar_chapter_word <- donhar_chapter %>%
unnest_tokens(word, text)
# Find document-word counts
donhar_word_counts <- donhar_chapter_word %>%
anti_join(stop_words) %>%
count(document, word, sort = TRUE) %>%
ungroup()
donhar_word_counts
# Cast into DTM format for LDA analysis
donhar_chapters_dtm <- donhar_word_counts %>%
cast_dtm(document, word, n)
tm::inspect(donhar_chapters_dtm)
donhar_chapters_lda <- LDA(donhar_chapters_dtm, k = 2, control = list(seed = 1234))
donhar_chapters_gamma <- tidy(donhar_chapters_lda, matrix = "gamma")
donhar_chapters_gamma
# First separate the document name into title and chapter
donhar_chapters_gamma <- donhar_chapters_gamma %>%
separate(document, c("title", "chapter"), sep = "_", convert = TRUE)
donhar_chapter_classifications <- donhar_chapters_gamma %>%
group_by(title, chapter) %>%
top_n(1, gamma) %>%
ungroup()
donhar_book_topics <- donhar_chapter_classifications %>%
count(title, topic) %>%
group_by(title) %>%
top_n(1, n) %>%
ungroup() %>%
transmute(consensus = title, topic)
donhar_chapter_classifications %>%
inner_join(donhar_book_topics, by = "topic") %>%
filter(title != consensus)
# Look document-word pairs were to see which words in each documents were assigned
# to a given topic
assignments_dh <- augment(donhar_chapters_lda, data = donhar_chapters_dtm)
assignments
assignments_dh <- augment(donhar_chapters_lda, data = donhar_chapters_dtm)
assignments_dh
# First separate the document name into title and chapter
donhar_chapters_gamma <- donhar_chapters_gamma %>%
separate(document, c("title", "chapter"), sep = "_", convert = TRUE)
# Divide into documents, each representing one chapter
donhar_chapter <- donhar %>%
mutate(author = ifelse(gutenberg_id==53368, "Donovan", "Hardy")) %>%
group_by(author) %>%
mutate(chapter = cumsum(str_detect(text, regex("^chapter ", ignore_case = TRUE)))) %>%
ungroup() %>%
filter(chapter > 0) %>%
unite(document, author, chapter)
# Split into words
donhar_chapter_word <- donhar_chapter %>%
unnest_tokens(word, text)
# Find document-word counts
donhar_word_counts <- donhar_chapter_word %>%
anti_join(stop_words) %>%
count(document, word, sort = TRUE) %>%
ungroup()
donhar_word_counts
donhar_chapters_dtm <- donhar_word_counts %>%
cast_dtm(document, word, n)
tm::inspect(donhar_dtm)
donhar_lda <- LDA(donhar_dtm, k = 6, control = list(seed = 1234))
donhar_topics <- tidy(donhar_lda, matrix = "beta")
head(donhar_topics, n = 10)
donhar_top_terms <- donhar_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
donhar_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free", ncol = 4) +
scale_y_reordered() +
theme_tufte(base_family = "Helvetica")
tidy_donhar <- donhar %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
## Count most common words in both
tidy_donhar %>%
count(word, sort = TRUE)
bookfreq_dh <- tidy_donhar %>%
mutate(author = ifelse(gutenberg_id==53368, "Donovan", "Hardy")) %>%
mutate(word = str_extract(word, "[a-z']+")) %>%
count(author, word) %>%
group_by(author) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(author, proportion)
ggplot(bookfreq_dh, aes(x = Donovan, y = Hardy, color = abs(Donovan - Hardy))) + ##absolute freq difference between Don and Har
geom_abline(color = "red", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "red") +
theme_tufte(base_family = "Helvetica") +
theme(legend.position="none",
strip.background = element_blank(),
strip.text.x = element_blank()) +
labs(x = "Donovan", y = "Hardy") +
coord_equal()
donhar <- donhar %>%
filter(!is.na(text))
# Divide into documents, each representing one chapter
donhar_chapter <- donhar %>%
mutate(author = ifelse(gutenberg_id==53368, "Donovan", "Hardy")) %>%
group_by(author) %>%
mutate(chapter = cumsum(str_detect(text, regex("^chapter ", ignore_case = TRUE)))) %>%
ungroup() %>%
filter(chapter > 0) %>%
unite(document, author, chapter)
# Split into words
donhar_chapter_word <- donhar_chapter %>%
unnest_tokens(word, text)
donhar_chapters_gamma <- donhar_chapters_gamma %>%
separate(document, c("title", "chapter"), sep = "_", convert = TRUE)
# Divide into documents, each representing one chapter
donhar_chapter <- donhar %>%
mutate(author = ifelse(gutenberg_id==53368, "Donovan", "Hardy")) %>%
group_by(author) %>%
mutate(chapter = cumsum(str_detect(text, regex("^chapter ", ignore_case = TRUE)))) %>%
ungroup() %>%
filter(chapter > 0) %>%
unite(document, author, chapter)
# Split into words
donhar_chapter_word <- donhar_chapter %>%
unnest_tokens(word, text)
# Find document-word counts
donhar_word_counts <- donhar_chapter_word %>%
anti_join(stop_words) %>%
count(document, word, sort = TRUE) %>%
ungroup()
donhar_word_counts
donhar_chapters_dtm <- donhar_word_counts %>%
cast_dtm(document, word, n)
# First separate the document name into title and chapter
donhar_chapters_gamma <- donhar_chapters_gamma %>%
separate(document, c("title", "chapter"), sep = "_", convert = TRUE)
donhar_chapters_gamma <- donhar_chapters_gamma %>%
separate(document, c("title", "chapter"), sep = "_", convert = TRUE)
donhar <- donhar %>%
filter(!is.na(text))
# Divide into documents, each representing one chapter
donhar_chapter <- donhar %>%
mutate(author = ifelse(gutenberg_id==53368, "Donovan", "Hardy")) %>%
group_by(author) %>%
mutate(chapter = cumsum(str_detect(text, regex("^chapter ", ignore_case = TRUE)))) %>%
ungroup() %>%
filter(chapter > 0) %>%
unite(document, author, chapter)
# Split into words
donhar_chapter_word <- donhar_chapter %>%
unnest_tokens(word, text)
# Find document-word counts
donhar_word_counts <- donhar_chapter_word %>%
anti_join(stop_words) %>%
count(document, word, sort = TRUE) %>%
ungroup()
donhar_word_counts
donhar_chapters_dtm <- donhar_word_counts %>%
cast_dtm(document, word, n)
tm::inspect(donhar_chapters_dtm)
donhar_chapters_lda <- LDA(donhar_chapters_dtm, k = 2, control = list(seed = 1234))
donhar_chapters_gamma <- tidy(donhar_chapters_lda, matrix = "gamma")
donhar_chapters_gamma
donhar_chapters_gamma <- donhar_chapters_gamma %>%
separate(document, c("title", "chapter"), sep = "_", convert = TRUE)
# First separate the document name into title and chapter
donhar_chapters_gamma <- donhar_chapters_gamma %>%
separate(document, c("title", "chapter"), sep = "_", convert = TRUE)
donhar <- gutenberg_download(c(53368, 35534),
meta_fields = "author") ##getting the dataframe
# First separate the document name into title and chapter
donhar_chapters_gamma <- donhar_chapters_gamma %>%
separate(document, c("title", "chapter"), sep = "_", convert = TRUE)
donhar_chapters_gamma <- tidy(donhar_chapters_lda, matrix = "gamma")
donhar_chapters_gamma
library(tidyverse) # loads dplyr, ggplot2, and others
library(stringr) # to handle text elements
library(tidytext) # includes set of functions useful for manipulating text
library(topicmodels) # to estimate topic models
library(gutenbergr) # to get text data
library(scales)
library(tm)
library(ggthemes) # to make your plots look nice
library(readr)
library(quanteda)
library(quanteda.textmodels)
library(dplyr)
donhar_chapters_gamma <- donhar_chapters_gamma %>%
separate(document, c("title", "chapter"), sep = "_", convert = TRUE)
donhar_chapter_classifications <- donhar_chapters_gamma %>%
group_by(title, chapter) %>%
top_n(1, gamma) %>%
ungroup()
donhar_book_topics <- donhar_chapter_classifications %>%
count(title, topic) %>%
group_by(title) %>%
top_n(1, n) %>%
ungroup() %>%
transmute(consensus = title, topic)
donhar_chapter_classifications %>%
inner_join(donhar_book_topics, by = "topic") %>%
filter(title != consensus)
# Look document-word pairs were to see which words in each documents were assigned
# to a given topic
assignments_dh <- augment(donhar_chapters_lda, data = donhar_chapters_dtm)
assignments_dh
assignments_dh <- assignments_dh %>%
separate(document, c("title", "chapter"), sep = "_", convert = TRUE) %>%
inner_join(donhar_book_topics, by = c(".topic" = "topic"))
assignments_dh %>%
count(title, consensus, wt = count) %>%
group_by(title) %>%
mutate(percent = n / sum(n)) %>%
ggplot(aes(consensus, title, fill = percent)) +
geom_tile() +
scale_fill_gradient2(high = "red", label = percent_format()) +
geom_text(aes(x = consensus, y = title, label = scales::percent(percent))) +
theme_tufte(base_family = "Helvetica") +
theme(axis.text.x = element_text(angle = 90, hjust = 1),
panel.grid = element_blank()) +
labs(x = "Book words assigned to",
y = "Book words came from",
fill = "% of assignments")
donhar <- gutenberg_download(c(57592, 35534),
meta_fields = "author") ##getting the dataframe
keyhar_words <- keyhar %>%
mutate(author = ifelse(gutenberg_id==57592, "Key", "Hardy")) %>% ##mutate() creates new column in the  dataset
unnest_tokens(word, text) %>% ##splits column into tokens aka single words, here we're interested in words and texts (built-in options)
filter(!is.na(word)) %>% ##is.na indicated missing elements,filter retains the rows that satisfy the condition, ! expression means NOT -> filter by non-missing words
count(author, word, sort = TRUE) %>%
ungroup() %>% ##ungrouping the data
anti_join(stop_words) ##removinh stop words from dataset
keyhar <- gutenberg_download(c(57592, 35534),
meta_fields = "author") ##getting the dataframe
keyhar_words <- keyhar %>%
mutate(author = ifelse(gutenberg_id==57592, "Key", "Hardy")) %>% ##mutate() creates new column in the  dataset
unnest_tokens(word, text) %>% ##splits column into tokens aka single words, here we're interested in words and texts (built-in options)
filter(!is.na(word)) %>% ##is.na indicated missing elements,filter retains the rows that satisfy the condition, ! expression means NOT -> filter by non-missing words
count(author, word, sort = TRUE) %>%
ungroup() %>% ##ungrouping the data
anti_join(stop_words) ##removinh stop words from dataset
keyhar_dtm <- keyhar_words %>%
cast_dtm(author, word, n)
tm::inspect(keyhar_dtm)
keyhar_lda <- LDA(keyhar_dtm, k = 6, control = list(seed = 1234))
keyhar_topics <- tidy(keyhar_lda, matrix = "beta")
head(keyhar_topics, n = 10)
keyhar <- gutenberg_download(c(57592, 35534),
meta_fields = "author") ##getting the dataframe
keyhar <- gutenberg_download(c(57592, 35534),
meta_fields = "author") ##getting the dataframe
keyhar_words <- keyhar %>%
mutate(author = ifelse(gutenberg_id==57592, "Key", "Hardy")) %>% ##mutate() creates new column in the  dataset
unnest_tokens(word, text) %>% ##splits column into tokens aka single words, here we're interested in words and texts (built-in options)
filter(!is.na(word)) %>% ##is.na indicated missing elements,filter retains the rows that satisfy the condition, ! expression means NOT -> filter by non-missing words
count(author, word, sort = TRUE) %>%
ungroup() %>% ##ungrouping the data
anti_join(stop_words) ##removinh stop words from dataset
keyhar_dtm <- keyhar_words %>%
cast_dtm(author, word, n)
tm::inspect(keyhar_dtm)
keyhar_lda <- LDA(keyhar_dtm, k = 6, control = list(seed = 1234))
keyhar_topics <- tidy(keyhar_lda, matrix = "beta")
head(keyhar_topics, n = 10)
keyhar_top_terms <- keyhar_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
keyhar_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free", ncol = 4) +
scale_y_reordered() +
theme_tufte(base_family = "Helvetica")
tidy_keyhar <- keyhar %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
## Count most common words in both
tidy_keyhar %>%
count(word, sort = TRUE)
bookfreq_kh <- tidy_keyhar %>%
mutate(author = ifelse(gutenberg_id==57592, "Key", "Hardy")) %>%
mutate(word = str_extract(word, "[a-z']+")) %>%
count(author, word) %>%
group_by(author) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(author, proportion)
ggplot(bookfreq_kh, aes(x = Key, y = Hardy, color = abs(Key - Hardy))) + ##absolute freq difference between Don and Har
geom_abline(color = "red", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "red") +
theme_tufte(base_family = "Helvetica") +
theme(legend.position="none",
strip.background = element_blank(),
strip.text.x = element_blank()) +
labs(x = "Key", y = "Hardy") +
coord_equal()
keyhar <- keyhar %>%
filter(!is.na(text))
# Divide into documents, each representing one chapter
keyhar_chapter <- keyhar %>%
mutate(author = ifelse(gutenberg_id==57592, "Key", "Hardy")) %>%
group_by(author) %>%
mutate(chapter = cumsum(str_detect(text, regex("^chapter ", ignore_case = TRUE)))) %>%
ungroup() %>%
filter(chapter > 0) %>%
unite(document, author, chapter)
# Split into words
keyhar_chapter_word <- keyhar_chapter %>%
unnest_tokens(word, text)
# Find document-word counts
keyhar_word_counts <- keyhar_chapter_word %>%
anti_join(stop_words) %>%
count(document, word, sort = TRUE) %>%
ungroup()
keyhar_word_counts
# Cast into DTM format for LDA analysis
keyhar_chapters_dtm <- keyhar_word_counts %>%
cast_dtm(document, word, n)
tm::inspect(keyhar_chapters_dtm)
keyhar_chapters_lda <- LDA(keyhar_chapters_dtm, k = 2, control = list(seed = 1234))
keyhar_chapters_gamma <- tidy(keyhar_chapters_lda, matrix = "gamma")
keyhar_chapters_gamma
# First separate the document name into title and chapter
keyhar_chapters_gamma <- keyhar_chapters_gamma %>%
separate(document, c("title", "chapter"), sep = "_", convert = TRUE)
keyhar_chapter_classifications <- keyhar_chapters_gamma %>%
group_by(title, chapter) %>%
top_n(1, gamma) %>%
ungroup()
keyhar_book_topics <- keyhar_chapter_classifications %>%
count(title, topic) %>%
group_by(title) %>%
top_n(1, n) %>%
ungroup() %>%
transmute(consensus = title, topic)
keyhar_chapter_classifications %>%
inner_join(keyhar_book_topics, by = "topic") %>%
filter(title != consensus)
# Look document-word pairs were to see which words in each documents were assigned
# to a given topic
assignments_kh <- augment(keyhar_chapters_lda, data = keyhar_chapters_dtm)
assignments_kh
assignments_kh <- assignments_kh %>%
separate(document, c("title", "chapter"), sep = "_", convert = TRUE) %>%
inner_join(keyhar_book_topics, by = c(".topic" = "topic"))
assignments_kh %>%
count(title, consensus, wt = count) %>%
group_by(title) %>%
mutate(percent = n / sum(n)) %>%
ggplot(aes(consensus, title, fill = percent)) +
geom_tile() +
scale_fill_gradient2(high = "red", label = percent_format()) +
geom_text(aes(x = consensus, y = title, label = scales::percent(percent))) +
theme_tufte(base_family = "Helvetica") +
theme(axis.text.x = element_text(angle = 90, hjust = 1),
panel.grid = element_blank()) +
labs(x = "Book words assigned to",
y = "Book words came from",
fill = "% of assignments")
# load in corpus of Keyhar text data.
corp <- corpus(keyhar, text_field = "text")
# use first 10 documents for example
documents <- corp[sample(1:30000,1000)]
# load in corpus of Keyhar text data.
corp <- corpus(keyhar, text_field = "text")
# use first 10 documents for example
documents <- corp[sample(1:3000,1000)]
# take a look at the document names
print(names(documents[1:10]))
# load in corpus of Keyhar text data.
corp <- corpus(keyhar, text_field = "text")
# use first 10 documents for example
documents <- corp[sample(1:5000,1000)]
# take a look at the document names
print(names(documents[1:10]))
# load in corpus of Keyhar text data.
corp <- corpus(keyhar, text_field = "text")
# use first 10 documents for example
documents <- corp[sample(1:20000,1000)]
# take a look at the document names
print(names(documents[1:10]))
# load in corpus of Keyhar text data.
corp <- corpus(keyhar, text_field = "text")
# use first 10 documents for example
documents <- corp[sample(1:30000,1000)]
# load in corpus of Keyhar text data.
corp <- corpus(keyhar, text_field = "text")
# use first 10 documents for example
documents <- corp[sample(1:20000,1000)]
# take a look at the document names
print(names(documents[1:10]))
preprocessed_documents <- factorial_preprocessing(
documents,
use_ngrams = TRUE,
infrequent_term_threshold = 0.2,
verbose = FALSE)
install.packages("devtools")
devtools::install_github("matthewjdenny/preText")
library(preText)
install.packages("devtools")
library(preText)
preprocessed_documents <- factorial_preprocessing(
documents,
use_ngrams = TRUE,
infrequent_term_threshold = 0.2,
verbose = FALSE)
preText_results <- preText(
preprocessed_documents,
dataset_name = "KeyHar text",
distance_method = "cosine",
num_comparisons = 20,
verbose = FALSE)
preText_score_plot(preText_results)
keyhar_topics
